<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Engineering</title>
    <style>
        body {
            font-family: monospace;
            margin: 20px;
            font-size: 0.9rem;
        }
         .container {
            max-width: 900px;
            margin: 40px auto;
            padding: 20px;
            background-color: #fff;
            border: 1px solid #ddd;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.05);
        }
        h1 {
            color: #363000;
            font-size: 1.2rem; /* You can change this number */
        }
        h2, h3 {
            color: #363050;
            font-size: 1.0rem;
        }
        .section {
            margin-bottom: 20px;
            font-size: 1.0rem;
        }
        .nav {
            margin-bottom: 20px;
            font-size: 0.8rem;
        }
        .topic-image {
            width: 25%; /* Adjust this percentage as you see fit */
            height: auto; /* This ensures the image doesn't get distorted */
            border-radius: 8px;
            margin-top: 15px;
        }
    </style>
</head>
<body>
    <div class="nav">
        <a href="index.html">Home</a>
    </div>
   
    <h1>Data Engineering</h1>

    <div class="container">

        <h1>Data Architecture & Engineering Portfolio</h1>
        
        <div class="section">
            <h2>Key Data Engineering Concepts üí°</h2>

            <h3>Event-Sourced and Immutable Data Model</h3>
            <p>A paradigm where application state is derived from a sequence of immutable events. Instead of storing the current state, every change is recorded as an event in an append-only log. This provides a <strong>complete audit trail</strong>, enables <strong>time travel</strong> to reconstruct past states, and enhances debugging by replaying events.</p>

            <h3>Salted-Hashed PII</h3>
            <p>Personally Identifiable Information (PII) protected by adding a unique, random "salt" before hashing. This one-way process creates a unique hash that protects against attacks like rainbow tables. Under GDPR, this is considered <strong>pseudonymization</strong>, not full anonymization, as the PII can be re-identified if the salt is known.</p>
            
            <h3>Hashed User IDs & Time-to-Live (TTL)</h3>
            <p>Technical tools for managing personal data. Hashing protects user IDs, while TTL policies automatically expire data. These methods help companies balance business needs with privacy regulations like GDPR, particularly for fulfilling the <strong>"Right to be Forgotten"</strong> (RTBF).</p>

            <h3>Architectural Patterns: Kappa vs. Lambda</h3>
            <p><strong>Kappa Architecture</strong> treats all data as a unified, real-time stream, simplifying systems by eliminating the batch layer. It's ideal for real-time analytics and event-driven applications. <strong>Lambda Architecture</strong> combines a batch layer for accurate historical processing with a speed layer for real-time insights, suited for systems needing both low-latency dashboards and comprehensive reports.</p>

            <h3>Exactly-Once Semantics (EOS)</h3>
            <p>A guarantee in distributed systems that a message or operation is processed a single, discrete time. This prevents both data loss and duplication, even during failures, by using techniques like idempotent producers/consumers and transactional messaging.</p>

            <h3>Watermarks in Streaming Data</h3>
            <p>Timestamps used to track event-time progress and manage late-arriving data. A watermark signals to a processing window that no more events older than its timestamp will arrive, allowing the system to finalize computations and clear state, preventing memory issues.</p>
            
            <h3>Distributed Query & Analytics Engines</h3>
            <p><strong>Trino (formerly PrestoSQL)</strong> is a distributed SQL query engine for running interactive analytics across multiple data sources simultaneously. <strong>Apache Pinot and Apache Druid</strong> are real-time OLAP databases designed for fast queries on large-scale datasets, with Pinot offering more flexibility for real-time updates (upserts) and Druid excelling at aggregation-heavy queries on time-series data.</p>
            
            <h3>Schema Contracts</h3>
            <p>Formal agreements defining the structure, behavior, and constraints of data exchanged between systems. They go beyond basic schemas to enforce data quality, manage sensitive information, and ensure predictable communication, acting as a crucial governance layer.</p>
        </div>

        <div class="section">
            <h2>Architectural Paradigms: Data Monolith vs. Data Mesh</h2>
            <div class="grid-container">
                <div class="grid-item">
                    <h3>Data Monolith: Failure Modes üëé</h3>
                    <ul>
                        <li><strong>Centralized, Domain-Agnostic:</strong> Creates a single point of failure, scaling bottlenecks, and friction in onboarding new data sources.</li>
                        <li><strong>Tightly-Coupled Pipeline:</strong> Technical decomposition leads to fragile, hard-to-maintain ETL jobs where changes cause cascading impacts.</li>
                        <li><strong>Siloed, Hyper-Specialized Team:</strong> The central platform team is disconnected from business context, creating organizational bottlenecks and impeding data ownership.</li>
                    </ul>
                </div>
                <div class="grid-item">
                    <h3>Data Mesh: Core Principles üëç</h3>
                    <ul>
                        <li><strong>Domain Ownership:</strong> Business domains own their analytical data end-to-end, aligning accountability with data production.</li>
                        <li><strong>Data as a Product:</strong> Data is treated as a product with defined consumers, SLOs, and quality standards, making it discoverable, trustworthy, and secure.</li>
                        <li><strong>Self-Serve Data Platform:</strong> A central platform provides infrastructure-as-a-service to enable domain autonomy and accelerate development.</li>
                        <li><strong>Federated Computational Governance:</strong> A council of domain owners and the platform team sets global rules, which are automated and embedded within the platform to balance autonomy with interoperability.</li>
                    </ul>
                </div>
            </div>
             <h3>Self-Serve Data Platform Capabilities</h3>
            <p>A successful self-serve platform in a Data Mesh provides the following capabilities to domain teams:</p>
            <ul>
                <li>Encryption for data at rest and in motion</li>
                <li>Data product versioning, schema management, and de-identification</li>
                <li>Unified data access control, logging, and lineage tracking</li>
                <li>Data pipeline implementation and orchestration tools</li>
                <li>A data product discovery catalog for publishing and registration</li>
                <li>Data quality metric collection and monitoring/alerting</li>
            </ul>
        </div>
        
        <div class="section">
            <h2>Risk Data Hybrid Ecosystem üõ°Ô∏è</h2>
            <p>
                The shift from a centralized HDFS data lake to a distributed hybrid architecture introduces significant risk to data lineage, a key component of model validation. In a Hortonworks environment, data lineage is managed within a contained ecosystem using tools like Apache Atlas. In the new architecture, a model's feature engineering pipeline may source data from on-premise systems, Google Cloud Storage (GCS) buckets, and Google BigQuery tables simultaneously. This fragmentation creates multiple points of failure for lineage tracking. A critical MRM control is to implement a unified data governance fabric, such as GCP Dataplex, to catalog and trace data assets across the hybrid environment, ensuring that the exact state and transformation logic of the data used for model training and execution can be reproduced for validation and regulatory audit, as required by regulations like SR 11-7.
                <br><br>
                A core principle of Model Risk Management (MRM) is ensuring that a model is reproducible. While the monolithic nature of a shared Hadoop cluster presented challenges, the move to OpenShift introduces both mitigants and new risks. The use of containerization (via Docker) and declarative YAML manifests for deployments drastically improves the technical reproducibility of a model's runtime environment. However, this shifts the risk to the container image lifecycle. The MRM framework must now incorporate controls for versioning Dockerfiles, scanning base images for vulnerabilities using GCP Container Analysis, and managing a private, immutable image repository (Artifact Registry). Without these controls, differences in library versions or environment variables between development, validation, and production containers can lead to non-reproducible model outputs and significant validation failures.
                <br><br>
                In the Hortonworks ecosystem, access to models and data was typically governed by perimeter security and internal controls like Apache Ranger policies. Migrating to a hybrid cloud model fundamentally changes the security risk by exposing models as microservices via APIs, managed through OpenShift Routes or GCP Cloud Endpoints. This introduces new risk vectors such as insecure API configurations, authentication/authorization failures, and network traversal vulnerabilities. A modern MRM framework must mandate a Zero Trust security model, requiring robust identity and access management (GCP IAM), strong authentication protocols (OAuth 2.0/OIDC), and network policies enforced by a service mesh like Istio to control both north-south and east-west traffic. The risk is the potential for model manipulation or denial-of-service attacks through its API endpoints.
                <br><br>
                The performance characteristics of risk models, particularly those used for real-time scoring or pricing, are considered part of their function. The distributed nature of the hybrid environment introduces significant performance volatility risk. A model executing on an OpenShift pod may experience unpredictable network latency when fetching data from an on-premise source via a Google Cloud Interconnect. This latency can cause model execution to breach its Service Level Agreement (SLA), rendering it ineffective for its business purpose. MRM must expand its scope to include rigorous performance testing that simulates these cross-environment data flows, re-benchmarking model execution times, and establishing new monitoring through tools like GCP Cloud Monitoring and Prometheus to detect performance degradation that could materially impact business decisions.
                <br><br>
                Auditing a traditional Hadoop environment often involved manual inspection of configuration files, access logs, and job histories. The shift to an Infrastructure-as-Code (IaC) and declarative model with OpenShift and GCP transforms this process. The primary source of truth for a model's environment and deployment configuration now resides in version-controlled repositories (e.g., Git) as Kubernetes YAML and Terraform files. The risk is that a subtle misconfiguration in a manifest could lead to a major compliance breach. The new MRM control becomes the implementation of a GitOps workflow, where every change to a model's production state is auditable, peer-reviewed, and automatically applied. This provides regulators and auditors with a complete, immutable log of changes, shifting the focus from manual server inspection to validating the integrity and security of the CI/CD pipeline and the declarative configurations themselves.
            </p>
        </div>

    </div>








































    
    <div class="section">
        <h2>Risk Data Hybrid Ecosystem</h2>
        <p>
            Risk modleing and shift from Hadoop to Hybrid Cluster
            <br><br>
            The shift from a centralized HDFS data lake to a distributed hybrid architecture introduces significant risk to data lineage, a key component of model validation. 
            In a Hortonworks environment, data lineage is  managed within a contained ecosystem using tools Apache Atlas. 
            In the new architecture, a model's feature engineering pipeline may source data from on-premise systems, Google Cloud Storage (GCS) buckets, and Google BigQuery tables simultaneously. 
            This fragmentation creates multiple points of failure for lineage tracking. 
            A critical MRM control is to implement a unified data governance fabric, GCP Dataplex, to catalog and trace data assets across the hybrid environment, 
            ensuring that the exact state and transformation logic of the data used for model training and execution can be reproduced for validation and regulatory audit, as required by regulations like SR 11-7.
            <br><br>
            Principle of MRM is ensuring that a model is reproducible. 
            While the monolithic nature of a shared Hadoop cluster presented challenges, the move to OpenShift introduces both mitigants and new risks. 
            The use of containerization (via Docker) and declarative YAML manifests for deployments drastically improves the technical reproducibility of a model's runtime environment. 
            However, this shifts the risk to the container image lifecycle. 
            The MRM framework must now incorporate controls for versioning Dockerfiles, scanning base images for vulnerabilities using GCP Container Analysis, and managing a private, immutable image repository (Artifact Registry). 
            Without these controls, differences in library versions or environment variables between development, validation, and production containers can lead to non-reproducible model outputs and significant validation failures.
            <br><br>
            In the Hortonworks ecosystem, access to models and data was typically governed by perimeter security and internal controls like Apache Ranger policies. 
            Migrating to a hybrid cloud model fundamentally changes the security risk by exposing models as microservices via APIs, managed through OpenShift Routes or GCP Cloud Endpoints. 
            This introduces new risk vectors such as insecure API configurations, authentication/authorization failures, and network traversal vulnerabilities. 
            A modern MRM framework must mandate a Zero Trust security model, requiring robust identity and access management (GCP IAM), strong authentication protocols (OAuth 2.0/OIDC), 
            and network policies enforced by a service mesh like Istio to control both north-south and east-west traffic. 
            The risk is the potential for model manipulation or denial-of-service attacks through its API endpoints.
            <br><br>
            The performance characteristics of risk models, particularly those used for real-time scoring or pricing, are considered part of their function. 
            The distributed nature of the hybrid environment introduces significant performance volatility risk. 
            A model executing on an OpenShift pod may experience unpredictable network latency when fetching data from an on-premise source via a Google Cloud Interconnect. 
            This latency can cause model execution to breach its Service Level Agreement (SLA), rendering it ineffective for its business purpose. 
            MRM must expand its scope to include rigorous performance testing that simulates these cross-environment data flows, re-benchmarking model execution times and establishing new monitoring through tools 
            like GCP Cloud Monitoring and Prometheus to detect performance degradation that could materially impact business decisions.
            <br><br>
            Auditing a traditional Hadoop environment often involved manual inspection of configuration files, access logs, and job histories. 
            The shift to an Infrastructure-as-Code (IaC) and declarative model with OpenShift and GCP transforms this process. 
            The primary source of truth for a model's environment and deployment configuration now resides in version-controlled repositories (e.g., Git) as Kubernetes YAML and Terraform files. 
            The risk is that a subtle misconfiguration in a manifest could lead to a major compliance breach. 
            The new MRM control becomes the implementation of a GitOps workflow, where every change to a model's production state is auditable, peer-reviewed, and automatically applied. 
            This provides regulators and auditors with a complete, immutable log of changes, shifting the focus from manual server inspection to validating the integrity and 
            security of the CI/CD pipeline and the declarative configurations themselves.
        </p>
    </div>

</body>
</html>
