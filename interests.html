<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Topics of Interest - Shrikant Khaire</title>
    <style>
        body {
            font-family: monospace;
            margin: 20px;
            font-size: 0.9rem;
        }
        h1 {
            font-size: 1.1rem; /* You can change this number */
        }
        h2, h3 {
            color: #333;
            font-size: 0.8rem;
        }
        .section {
            margin-bottom: 20px;
            font-size: 0.8rem;
        }
        .nav {
            margin-bottom: 20px;
            font-size: 0.8rem;
        }
        .topic-image {
            width: 25%; /* Adjust this percentage as you see fit */
            height: auto; /* This ensures the image doesn't get distorted */
            border-radius: 8px;
            margin-top: 15px;
        }
    </style>
</head>
<body>
    <div class="nav">
        <a href="index.html">Home</a>
    </div>

    <h1>Topics of Interest</h1>

    <div class="section">
        <h2>Generative AI & Large Language Models</h2>
        <p>
            Experience with the H200 GPU cluster:

            <p style="line-height: 2;">This paragraph has double line spacing.</p>
            
            "190 PFLOPS H200 GPU"

            
            Computational Throughput: 190 PFLOPS specifies a computational rate of 190 petaflops. A petaflop is a standard metric in high-performance computing (HPC) equivalent to 10^15 floating-point operations per second (FLOPS). This system can therefore execute 190 quadrillion such operations per second.

            
            Processing Hardware: The H200 is a specific model of Graphics Processing Unit (GPU) manufactured by NVIDIA. It is based on the "Hopper" microarchitecture. A primary hardware feature is its 141 GB of HBM3e (High-Bandwidth Memory 3e), which provides an extremely high data transfer rate, a critical factor for performance in data-intensive workloads.

            
            System Architecture: The 190 PFLOPS denotes the aggregate peak performance of a 48 H200-GPU cards interconnected via high-speed fabric technologies like NVIDIA NVLink to function as a unified computational resource.

            
            FP8 (8-bit Floating Point): ~4 PFLOPS (3,958 TFLOPS

            48 GPUs Ã— 3.958 PFLOPS/GPU (FP8) = 190 PFLOPS
        </p>
        <h3>Analogy for GPUDirect</h3>
        <p>
    To understand how GPUDirect works, think of your computer's components like an office. Normally, data has to take a winding path from storage (the "Mailroom") to the CPU (the "Secretary") before finally reaching the GPU (the "Graphics Department"). This creates a bottleneck. GPUDirect acts like an express elevator, allowing data to bypass the CPU and travel directly from storage to the GPU, dramatically speeding up the process.
        </p>

<img src="images/GPUDirect Analogy.png" alt="An analogy explaining how GPUDirect creates a direct data path, bypassing the CPU." class="topic-image">
    </div>

    <div class="section">
        <h2>Data Engineering at Scale</h2>
        <p>
            Placeholder to write about the shift from traditional data warehouses to hybrid cloud environments or the role of technologies like Apache Iceberg.
        </p>
    </div>

    <div class="section">
        <h2>MLOps and Model Lifecycle Management</h2>
        <p>
            Write about the importance of automation in the model lifecycle, from development and validation to deployment and monitoring.
        </p>
    </div>

</body>
</html>
