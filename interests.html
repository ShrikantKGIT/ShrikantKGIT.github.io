<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Topics of Interest - Shrikant Khaire</title>
    <style>
        body {
            font-family: monospace;
            margin: 20px;
            font-size: 0.9rem;
        }
        h1 {
            color: #363000;
            font-size: 1.2rem; /* You can change this number */
        }
        h2, h3 {
            color: #363050;
            font-size: 1.0rem;
        }
        .section {
            margin-bottom: 20px;
            font-size: 1.0rem;
        }
        .nav {
            margin-bottom: 20px;
            font-size: 0.8rem;
        }
        .topic-image {
            width: 25%; /* Adjust this percentage as you see fit */
            height: auto; /* This ensures the image doesn't get distorted */
            border-radius: 8px;
            margin-top: 15px;
        }
    </style>
</head>
<body>
    <div class="nav">
        <a href="index.html">Home</a>
    </div>

    <div style="margin-bottom: 25px;">
        <a href="dataengineering.html">Data Engineering</a>
    </div>

    
    <h1>Topics of Interest</h1>

    <div class="section">
        <h2>Generative AI & Large Language Models</h2>
        <p>
            Experience with the H200 GPU cluster:
            <p style="line-height: 2;"> </p>
            "190 PFLOPS H200 GPU"
            <br>
            Computational Throughput: 190 PFLOPS specifies a computational rate of 190 petaflops. A petaflop is a standard metric in high-performance computing (HPC) equivalent to 10^15 floating-point operations per second (FLOPS). This system can therefore execute 190 quadrillion such operations per second.
            <br>
            Processing Hardware: The H200 is a specific model of Graphics Processing Unit (GPU) manufactured by NVIDIA. It is based on the "Hopper" microarchitecture. A primary hardware feature is its 141 GB of HBM3e (High-Bandwidth Memory 3e), which provides an extremely high data transfer rate, a critical factor for performance in data-intensive workloads.
            <br>
            System Architecture: The 190 PFLOPS denotes the aggregate peak performance of a 48 H200-GPU cards interconnected via high-speed NVIDIA NVLink to function as a unified computational resource.
            <br>
            FP8 (8-bit Floating Point): ~4 PFLOPS (3,958 TFLOPS)
            <br>
            48 GPUs × 3.958 PFLOPS/GPU (FP8) = 190 PFLOPS
            <p style="line-height: 2;"> </p>
            NVIDIA NVLink
            <br>
            <br>
            NVIDIA NVLink is a proprietary, chip-to-chip interconnect protocol that provides a high-density, low-latency physical layer for GPU-to-GPU and GPU-to-CPU communication. 
            The generation used with Hopper H200 GPUs is NVLink 4.0, which utilizes 18 differential pairs per link, each operating at 50 Gbit/s to provide 25 GB/s of unidirectional bandwidth. 
            With 18 links per GPU, this results in a total bidirectional bandwidth of 900 GB/s per GPU. This is over 7 times the 128 GB/s bidirectional bandwidth of a standard PCIe 5.0 x16 slot. 
            In multi-GPU server nodes, these links connect to a 3rd-generation NVSwitch, a hardware fabric that enables all-to-all, non-blocking communication between the GPUs within the node and can extend connectivity across nodes.
            <br>
            NVLink's primary function is to establish a single, coherent memory address space for up to 256 GPUs, enabling direct memory access (DMA) between their individual VRAMs while bypassing the host CPU and PCIe bus. 
            This architecture drastically reduces latency and increases bandwidth for collective communication operations (e.g., All-Reduce), which are critical performance bottlenecks in distributed computing. 
            Consequently, it enables the efficient implementation of both model parallelism for neural networks that exceed single-GPU memory and large-scale data parallelism required by cutting-edge AI and HPC workloads.
        </p>
        
        <h3>GPUDirect</h3>
        <p>
            To understand how GPUDirect works, think of your computer's components like an office. 
            Normally, data has to take a winding path from storage (the "Mailroom") to the CPU (the "Secretary") before finally reaching the GPU (the "Graphics Department"). 
            This creates a bottleneck. GPUDirect acts like an express elevator, allowing data to bypass the CPU and travel directly from storage to the GPU, dramatically speeding up the process.
        </p>
            <img src="images/GPUDirect Analogy.png" alt="An analogy explaining how GPUDirect creates a direct data path, bypassing the CPU." class="topic-image">
        <p>
            <br>
            Parameter Storage and Model Loading Requirements
            <br>
            The memory footprint of a model is determined by its parameter count. Storing a model for inference using bfloat16 precision requires 2 bytes per parameter.
            <br>
            A 20-billion parameter model (GPB 20B) requires 20B × 2 bytes = 40 GB of VRAM. This fits comfortably within the 141 GB capacity of a single H200 GPU.
            A 120-billion parameter model (GPT 120B) requires 120B × 2 bytes = 240 GB of VRAM. 
            This exceeds the capacity of a single H200, necessitating model parallelism, where the model is partitioned across at least two interconnected GPUs.
        </p>

        <p>
            Accelerated Data Ingestion via GPUDirect Storage and RDMA
            <br>
            Fine-tuning GPT-120B is frequently an I/O-bound process where performance is limited by the data ingestion pipeline rather than raw compute. 
            NVIDIA's GPUDirect Storage technology is used to mitigate this by eliminating the traditional data path bottleneck, where data is staged in a CPU-managed "bounce buffer" in system RAM before being 
            copied to the GPU's VRAM (Storage -> System RAM -> VRAM). 
            Instead, GPUDirect provides a kernel-level framework that allows the GPU's internal Direct Memory Access (DMA) engine to directly control data transfers from storage.
            <br>
            This direct path is typically facilitated over a high-speed network fabric InfiniBand) using the RDMA protocol. 
            To interface with S3 HPOS, this architecture  utilizes an intermediary GPUDirect-aware parallel file system, which can present the object data as a POSIX-compliant file system. 
            By completely bypassing the CPU and system RAM for data payload transfers, this method drastically reduces I/O latency, frees up CPU cycles, and saturates the H200's memory bandwidth. 
            The result is a substantial reduction in epoch completion time, as the GPU cores are kept consistently fed with data, maximizing their utilization and accelerating the overall convergence of the fine-tuning task.
        </p>
    </div>

    <div class="section">
        <h2>Data Engineering at Scale</h2>
        <p>
            Risk modleing and shift from Hadoop to Hybrid Cluster
            <br><br>
            The shift from a centralized HDFS data lake to a distributed hybrid architecture introduces significant risk to data lineage, a key component of model validation. 
            In a Hortonworks environment, data lineage is  managed within a contained ecosystem using tools Apache Atlas. 
            In the new architecture, a model's feature engineering pipeline may source data from on-premise systems, Google Cloud Storage (GCS) buckets, and Google BigQuery tables simultaneously. 
            This fragmentation creates multiple points of failure for lineage tracking. 
            A critical MRM control is to implement a unified data governance fabric, GCP Dataplex, to catalog and trace data assets across the hybrid environment, 
            ensuring that the exact state and transformation logic of the data used for model training and execution can be reproduced for validation and regulatory audit, as required by regulations like SR 11-7.
            <br><br>
            Principle of MRM is ensuring that a model is reproducible. 
            While the monolithic nature of a shared Hadoop cluster presented challenges, the move to OpenShift introduces both mitigants and new risks. 
            The use of containerization (via Docker) and declarative YAML manifests for deployments drastically improves the technical reproducibility of a model's runtime environment. 
            However, this shifts the risk to the container image lifecycle. 
            The MRM framework must now incorporate controls for versioning Dockerfiles, scanning base images for vulnerabilities using GCP Container Analysis, and managing a private, immutable image repository (Artifact Registry). 
            Without these controls, differences in library versions or environment variables between development, validation, and production containers can lead to non-reproducible model outputs and significant validation failures.
            <br><br>
            In the Hortonworks ecosystem, access to models and data was typically governed by perimeter security and internal controls like Apache Ranger policies. 
            Migrating to a hybrid cloud model fundamentally changes the security risk by exposing models as microservices via APIs, managed through OpenShift Routes or GCP Cloud Endpoints. 
            This introduces new risk vectors such as insecure API configurations, authentication/authorization failures, and network traversal vulnerabilities. 
            A modern MRM framework must mandate a Zero Trust security model, requiring robust identity and access management (GCP IAM), strong authentication protocols (OAuth 2.0/OIDC), 
            and network policies enforced by a service mesh like Istio to control both north-south and east-west traffic. 
            The risk is the potential for model manipulation or denial-of-service attacks through its API endpoints.
            <br><br>
            The performance characteristics of risk models, particularly those used for real-time scoring or pricing, are considered part of their function. 
            The distributed nature of the hybrid environment introduces significant performance volatility risk. 
            A model executing on an OpenShift pod may experience unpredictable network latency when fetching data from an on-premise source via a Google Cloud Interconnect. 
            This latency can cause model execution to breach its Service Level Agreement (SLA), rendering it ineffective for its business purpose. 
            MRM must expand its scope to include rigorous performance testing that simulates these cross-environment data flows, re-benchmarking model execution times and establishing new monitoring through tools 
            like GCP Cloud Monitoring and Prometheus to detect performance degradation that could materially impact business decisions.
            <br><br>
            Auditing a traditional Hadoop environment often involved manual inspection of configuration files, access logs, and job histories. 
            The shift to an Infrastructure-as-Code (IaC) and declarative model with OpenShift and GCP transforms this process. 
            The primary source of truth for a model's environment and deployment configuration now resides in version-controlled repositories (e.g., Git) as Kubernetes YAML and Terraform files. 
            The risk is that a subtle misconfiguration in a manifest could lead to a major compliance breach. 
            The new MRM control becomes the implementation of a GitOps workflow, where every change to a model's production state is auditable, peer-reviewed, and automatically applied. 
            This provides regulators and auditors with a complete, immutable log of changes, shifting the focus from manual server inspection to validating the integrity and 
            security of the CI/CD pipeline and the declarative configurations themselves.
        </p>
    </div>

    <div class="section">
        <h2>MLOps and Model Lifecycle Management</h2>
        <p>
            Write about the importance of automation in the model lifecycle, from development and validation to deployment and monitoring.
        </p>
    </div>

</body>
</html>
