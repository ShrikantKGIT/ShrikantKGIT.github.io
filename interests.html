<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Topics of Interest - Shrikant Khaire</title>
    <style>
        body {
            font-family: monospace;
            margin: 20px;
            font-size: 0.9rem;
        }
        h1 {
            color: #363000;
            font-size: 1.2rem; /* You can change this number */
        }
        h2, h3 {
            color: #363050;
            font-size: 1.0rem;
        }
        .section {
            margin-bottom: 20px;
            font-size: 1.0rem;
        }
        .nav {
            margin-bottom: 20px;
            font-size: 0.8rem;
        }
        .topic-image {
            width: 25%; /* Adjust this percentage as you see fit */
            height: auto; /* This ensures the image doesn't get distorted */
            border-radius: 8px;
            margin-top: 15px;
        }
    </style>
</head>
<body>
    <div class="nav">
        <a href="index.html">Home</a>
    </div>

    <div style="margin-bottom: 25px;">
        <a href="genaiengineering.html">Generative AI Engineering</a>
    </div>
    
    <div style="margin-bottom: 25px;">
        <a href="dataengineering.html">Data Engineering</a>
    </div>

    
    <h1>Topics of Interest</h1>

    <div class="section">
        <h2>Generative AI & Large Language Models</h2>
        <p>
            Experience with the H200 GPU cluster:
            <p style="line-height: 2;"> </p>
            "190 PFLOPS H200 GPU"
            <br>
            Computational Throughput: 190 PFLOPS specifies a computational rate of 190 petaflops. A petaflop is a standard metric in high-performance computing (HPC) equivalent to 10^15 floating-point operations per second (FLOPS). This system can therefore execute 190 quadrillion such operations per second.
            <br>
            Processing Hardware: The H200 is a specific model of Graphics Processing Unit (GPU) manufactured by NVIDIA. It is based on the "Hopper" microarchitecture. A primary hardware feature is its 141 GB of HBM3e (High-Bandwidth Memory 3e), which provides an extremely high data transfer rate, a critical factor for performance in data-intensive workloads.
            <br>
            System Architecture: The 190 PFLOPS denotes the aggregate peak performance of a 48 H200-GPU cards interconnected via high-speed NVIDIA NVLink to function as a unified computational resource.
            <br>
            FP8 (8-bit Floating Point): ~4 PFLOPS (3,958 TFLOPS)
            <br>
            48 GPUs × 3.958 PFLOPS/GPU (FP8) = 190 PFLOPS
            <p style="line-height: 2;"> </p>
            NVIDIA NVLink
            <br>
            <br>
            NVIDIA NVLink is a proprietary, chip-to-chip interconnect protocol that provides a high-density, low-latency physical layer for GPU-to-GPU and GPU-to-CPU communication. 
            The generation used with Hopper H200 GPUs is NVLink 4.0, which utilizes 18 differential pairs per link, each operating at 50 Gbit/s to provide 25 GB/s of unidirectional bandwidth. 
            With 18 links per GPU, this results in a total bidirectional bandwidth of 900 GB/s per GPU. This is over 7 times the 128 GB/s bidirectional bandwidth of a standard PCIe 5.0 x16 slot. 
            In multi-GPU server nodes, these links connect to a 3rd-generation NVSwitch, a hardware fabric that enables all-to-all, non-blocking communication between the GPUs within the node and can extend connectivity across nodes.
            <br>
            NVLink's primary function is to establish a single, coherent memory address space for up to 256 GPUs, enabling direct memory access (DMA) between their individual VRAMs while bypassing the host CPU and PCIe bus. 
            This architecture drastically reduces latency and increases bandwidth for collective communication operations (e.g., All-Reduce), which are critical performance bottlenecks in distributed computing. 
            Consequently, it enables the efficient implementation of both model parallelism for neural networks that exceed single-GPU memory and large-scale data parallelism required by cutting-edge AI and HPC workloads.
        </p>
        
        <h3>GPUDirect</h3>
        <p>
            To understand how GPUDirect works, think of your computer's components like an office. 
            Normally, data has to take a winding path from storage (the "Mailroom") to the CPU (the "Secretary") before finally reaching the GPU (the "Graphics Department"). 
            This creates a bottleneck. GPUDirect acts like an express elevator, allowing data to bypass the CPU and travel directly from storage to the GPU, dramatically speeding up the process.
        </p>
            <img src="images/GPUDirect Analogy.png" alt="An analogy explaining how GPUDirect creates a direct data path, bypassing the CPU." class="topic-image">
        <p>
            <br>
            Parameter Storage and Model Loading Requirements
            <br>
            The memory footprint of a model is determined by its parameter count. Storing a model for inference using bfloat16 precision requires 2 bytes per parameter.
            <br>
            A 20-billion parameter model (GPB 20B) requires 20B × 2 bytes = 40 GB of VRAM. This fits comfortably within the 141 GB capacity of a single H200 GPU.
            A 120-billion parameter model (GPT 120B) requires 120B × 2 bytes = 240 GB of VRAM. 
            This exceeds the capacity of a single H200, necessitating model parallelism, where the model is partitioned across at least two interconnected GPUs.
        </p>

        <p>
            Accelerated Data Ingestion via GPUDirect Storage and RDMA
            <br>
            Fine-tuning GPT-120B is frequently an I/O-bound process where performance is limited by the data ingestion pipeline rather than raw compute. 
            NVIDIA's GPUDirect Storage technology is used to mitigate this by eliminating the traditional data path bottleneck, where data is staged in a CPU-managed "bounce buffer" in system RAM before being 
            copied to the GPU's VRAM (Storage -> System RAM -> VRAM). 
            Instead, GPUDirect provides a kernel-level framework that allows the GPU's internal Direct Memory Access (DMA) engine to directly control data transfers from storage.
            <br>
            This direct path is typically facilitated over a high-speed network fabric InfiniBand) using the RDMA protocol. 
            To interface with S3 HPOS, this architecture  utilizes an intermediary GPUDirect-aware parallel file system, which can present the object data as a POSIX-compliant file system. 
            By completely bypassing the CPU and system RAM for data payload transfers, this method drastically reduces I/O latency, frees up CPU cycles, and saturates the H200's memory bandwidth. 
            The result is a substantial reduction in epoch completion time, as the GPU cores are kept consistently fed with data, maximizing their utilization and accelerating the overall convergence of the fine-tuning task.
        </p>
    </div>

</body>
</html>
